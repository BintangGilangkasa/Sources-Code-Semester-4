{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc9d10c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# utils_pcn.py\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy.signal import convolve2d\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from torchvision.datasets import FashionMNIST\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0fa21863",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_fashion_mnist(n_train=10000, n_test=2000):\n",
    "    transform = transforms.ToTensor()\n",
    "    trainset = FashionMNIST(root='./data', train=True, download=True, transform=transform)\n",
    "    testset = FashionMNIST(root='./data', train=False, download=True, transform=transform)\n",
    "    X_train = trainset.data[:n_train].numpy().astype(np.float32)\n",
    "    y_train = np.array(trainset.targets[:n_train])\n",
    "    X_test = testset.data[:n_test].numpy().astype(np.float32)\n",
    "    y_test = np.array(testset.targets[:n_test])\n",
    "    return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a04bc700",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_random_patches(images, patch_size=7, stride=1, max_patches=50000):\n",
    "    n, h, w = images.shape\n",
    "    k = patch_size\n",
    "    patch_vectors = []\n",
    "    for idx in range(n):\n",
    "        for i in range(0, h - k + 1, stride):\n",
    "            for j in range(0, w - k + 1, stride):\n",
    "                patch = images[idx, i:i + k, j:j + k]\n",
    "                patch = patch - np.mean(patch)\n",
    "                patch_vectors.append(patch.flatten())\n",
    "                if len(patch_vectors) >= max_patches:\n",
    "                    return np.array(patch_vectors).T\n",
    "    return np.array(patch_vectors).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3ec9eb30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn_pca_filters(patches, num_filters, patch_size):\n",
    "    pca = PCA(n_components=num_filters)\n",
    "    pca.fit(patches.T)\n",
    "    filters = pca.components_.reshape((num_filters, patch_size, patch_size))\n",
    "    return filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f2be015d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convolve_images(images, filters):\n",
    "    feature_maps = []\n",
    "    for f in filters:\n",
    "        fmaps = [convolve2d(img, f, mode='valid') for img in images]\n",
    "        feature_maps.append(np.stack(fmaps))\n",
    "    return feature_maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f9f2faaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_hashing(feature_maps):\n",
    "    bin_stack = np.stack([(fm > 0).astype(np.uint8) for fm in feature_maps], axis=0)\n",
    "    powers = 2 ** np.arange(bin_stack.shape[0])[::-1].reshape((-1, 1, 1, 1))\n",
    "    hashed = np.sum(bin_stack * powers, axis=0)\n",
    "    return hashed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2aa80b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def block_histogram(images, block_size=(7, 7), num_bins=256, overlap=0.5):\n",
    "    n, h, w = images.shape\n",
    "    bh, bw = block_size\n",
    "    step_h = int(bh * (1 - overlap))\n",
    "    step_w = int(bw * (1 - overlap))\n",
    "    features = []\n",
    "    for img in images:\n",
    "        blocks = []\n",
    "        for i in range(0, h - bh + 1, step_h):\n",
    "            for j in range(0, w - bw + 1, step_w):\n",
    "                block = img[i:i + bh, j:j + bw]\n",
    "                hist, _ = np.histogram(block, bins=num_bins, range=(0, num_bins))\n",
    "                blocks.extend(hist)\n",
    "        features.append(np.array(blocks))\n",
    "    return np.stack(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "41d5ac73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(features_train, y_train, features_test, y_test):\n",
    "    clf = LinearSVC(max_iter=10000)\n",
    "    clf.fit(features_train, y_train)\n",
    "    y_pred = clf.predict(features_test)\n",
    "    return accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "756d8b05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“¦ Loading Fashion-MNIST...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26.4M/26.4M [04:12<00:00, 105kB/s] \n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29.5k/29.5k [00:02<00:00, 12.5kB/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4.42M/4.42M [00:38<00:00, 116kB/s] \n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5.15k/5.15k [00:00<00:00, 5.25MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ§© Stage-1 Patch Extraction...\n",
      "\n",
      "ğŸ”§ Stage-1 PCA Filter Training...\n",
      "\n",
      "ğŸ›ï¸ Stage-1 Convolution...\n",
      "\n",
      "ğŸ§  Stage-2 Patch Extraction...\n",
      "\n",
      "ğŸ”§ Stage-2 PCA Filter Training...\n",
      "\n",
      "ğŸ›ï¸ Stage-2 Convolution...\n",
      "\n",
      "ğŸ” Binary Hashing + Histogram (Train)...\n",
      "\n",
      "ğŸ§ª Convolution on Test Set...\n",
      "\n",
      "ğŸ·ï¸ Training + Evaluating Linear SVM...\n",
      "\n",
      "âœ… Final Test Accuracy on Fashion-MNIST: 10.35%\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Jalankan pipeline PCN untuk Fashion-MNIST\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"\\nğŸ“¦ Loading Fashion-MNIST...\")\n",
    "X_train, y_train, X_test, y_test = load_fashion_mnist()\n",
    "\n",
    "print(\"\\nğŸ§© Stage-1 Patch Extraction...\")\n",
    "patches1 = extract_random_patches(X_train, patch_size=7, stride=1, max_patches=50000)\n",
    "\n",
    "print(\"\\nğŸ”§ Stage-1 PCA Filter Training...\")\n",
    "filters1 = learn_pca_filters(patches1, num_filters=6, patch_size=7)\n",
    "\n",
    "print(\"\\nğŸ›ï¸ Stage-1 Convolution...\")\n",
    "fmap1 = convolve_images(X_train, filters1)\n",
    "\n",
    "print(\"\\nğŸ§  Stage-2 Patch Extraction...\")\n",
    "fmap1_concat = np.concatenate(fmap1, axis=0)[:3000]\n",
    "patches2 = extract_random_patches(fmap1_concat, patch_size=7, stride=1, max_patches=50000)\n",
    "\n",
    "print(\"\\nğŸ”§ Stage-2 PCA Filter Training...\")\n",
    "filters2 = learn_pca_filters(patches2, num_filters=11, patch_size=7)\n",
    "\n",
    "print(\"\\nğŸ›ï¸ Stage-2 Convolution...\")\n",
    "fmap2 = []\n",
    "for fmap in fmap1:\n",
    "    filtered = convolve_images(fmap, filters2)\n",
    "    fmap2.append(filtered)\n",
    "fmap2 = [f for group in fmap2 for f in group]\n",
    "\n",
    "print(\"\\nğŸ” Binary Hashing + Histogram (Train)...\")\n",
    "hashed_train = binary_hashing(fmap2)\n",
    "features_train = block_histogram(hashed_train, block_size=(7, 7), overlap=0.5)\n",
    "\n",
    "print(\"\\nğŸ§ª Convolution on Test Set...\")\n",
    "fmap1_test = convolve_images(X_test, filters1)\n",
    "fmap2_test = []\n",
    "for fmap in fmap1_test:\n",
    "    filtered = convolve_images(fmap, filters2)\n",
    "    fmap2_test.append(filtered)\n",
    "fmap2_test = [f for group in fmap2_test for f in group]\n",
    "\n",
    "hashed_test = binary_hashing(fmap2_test)\n",
    "features_test = block_histogram(hashed_test, block_size=(7, 7), overlap=0.5)\n",
    "\n",
    "print(\"\\nğŸ·ï¸ Training + Evaluating Linear SVM...\")\n",
    "acc = train_and_evaluate(features_train, y_train, features_test, y_test)\n",
    "print(f\"\\nâœ… Final Test Accuracy on Fashion-MNIST: {acc * 100:.2f}%\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
