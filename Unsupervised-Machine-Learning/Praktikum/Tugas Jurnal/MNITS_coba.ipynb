{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "73b2015c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "da12e911",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === LOAD CSV DATA ===\n",
    "def load_csv_images(csv_path, img_size=(28, 28), max_samples=None):\n",
    "    df = pd.read_csv(csv_path)\n",
    "    y = df['label'].values\n",
    "    X = df.drop('label', axis=1).values.astype(np.float32)\n",
    "    if max_samples:\n",
    "        X = X[:max_samples]\n",
    "        y = y[:max_samples]\n",
    "    X = X.reshape((-1, img_size[0], img_size[1]))\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc5be477",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === PATCH EXTRACTION ===\n",
    "def extract_random_patches(images, patch_size=7, stride=1, max_patches=50000):\n",
    "    patches = []\n",
    "    for img in images:\n",
    "        for i in range(0, img.shape[0] - patch_size + 1, stride):\n",
    "            for j in range(0, img.shape[1] - patch_size + 1, stride):\n",
    "                patch = img[i:i+patch_size, j:j+patch_size]\n",
    "                patch = patch - np.mean(patch)\n",
    "                patches.append(patch.flatten())\n",
    "                if len(patches) >= max_patches:\n",
    "                    return np.array(patches)\n",
    "    return np.array(patches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8cfa18cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === PCA FILTER LEARNING ===\n",
    "def learn_pca_filters(patches, num_filters, patch_size):\n",
    "    pca = PCA(n_components=num_filters)\n",
    "    pca.fit(patches)\n",
    "    filters = pca.components_.reshape((num_filters, patch_size, patch_size))\n",
    "    return filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "893564fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CONVOLVE IMAGES ===\n",
    "def convolve_images(images, filters):\n",
    "    fmap_all = []\n",
    "    for img in images:\n",
    "        maps = [cv2.filter2D(img, -1, f) for f in filters]\n",
    "        fmap_all.append(np.stack(maps))\n",
    "    return np.array(fmap_all)  # shape: (N, num_filters, H, W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "435e88b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === BINARY HASHING + HISTOGRAM ===\n",
    "def binary_hashing(feature_maps):\n",
    "    bin_stack = (feature_maps > 0).astype(np.uint8)\n",
    "    powers = 2 ** np.arange(bin_stack.shape[1])[::-1].reshape((-1, 1, 1))\n",
    "    hashed = np.sum(bin_stack * powers, axis=1)\n",
    "    return hashed\n",
    "\n",
    "def block_histogram(images, block_size=(7, 7), num_bins=256, overlap=0.5):\n",
    "    n, h, w = images.shape\n",
    "    bh, bw = block_size\n",
    "    step_h = int(bh * (1 - overlap))\n",
    "    step_w = int(bw * (1 - overlap))\n",
    "    features = []\n",
    "    for img in images:\n",
    "        blocks = []\n",
    "        for i in range(0, h - bh + 1, step_h):\n",
    "            for j in range(0, w - bw + 1, step_w):\n",
    "                block = img[i:i + bh, j:j + bw]\n",
    "                hist, _ = np.histogram(block, bins=num_bins, range=(0, num_bins))\n",
    "                blocks.extend(hist)\n",
    "        features.append(np.array(blocks))\n",
    "    return np.stack(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca9e55af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Loading CSV data...\n",
      "🧩 Extracting stage-1 patches...\n",
      "🔧 Training stage-1 PCA filters...\n",
      "🎛 Convolution stage-1...\n",
      "🧠 Extracting stage-2 patches...\n",
      "🔧 Training stage-2 PCA filters...\n",
      "🎛 Convolution stage-2...\n",
      "🔐 Binary hashing + histogram (train)...\n",
      "🔐 Binary hashing + histogram (test)...\n",
      "🏷 Training Linear SVM...\n",
      "✅ Final Test Accuracy: 89.50%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\62822\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\svm\\_base.py:1243: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# === RUN PCN ON CSV ===\n",
    "def run_pcn_from_csv(csv_path):\n",
    "    print(\"🔍 Loading CSV data...\")\n",
    "    X, y = load_csv_images(csv_path, max_samples=1000)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8, stratify=y, random_state=42)\n",
    "\n",
    "    print(\"🧩 Extracting stage-1 patches...\")\n",
    "    patches1 = extract_random_patches(X_train, patch_size=7, stride=1, max_patches=50000)\n",
    "\n",
    "    print(\"🔧 Training stage-1 PCA filters...\")\n",
    "    filters1 = learn_pca_filters(patches1, num_filters=6, patch_size=7)\n",
    "\n",
    "    print(\"🎛 Convolution stage-1...\")\n",
    "    fmap1_train = convolve_images(X_train, filters1)\n",
    "    fmap1_test = convolve_images(X_test, filters1)\n",
    "\n",
    "    print(\"🧠 Extracting stage-2 patches...\")\n",
    "    combined_maps = np.array([np.sum(fm, axis=0) for fm in fmap1_train])\n",
    "    patches2 = extract_random_patches(combined_maps, patch_size=7, stride=1, max_patches=50000)\n",
    "\n",
    "    print(\"🔧 Training stage-2 PCA filters...\")\n",
    "    filters2 = learn_pca_filters(patches2, num_filters=11, patch_size=7)\n",
    "\n",
    "    print(\"🎛 Convolution stage-2...\")\n",
    "    fmap2_train = np.array([np.stack([cv2.filter2D(np.sum(fm, axis=0), -1, f) for f in filters2]) for fm in fmap1_train])\n",
    "    fmap2_test = np.array([np.stack([cv2.filter2D(np.sum(fm, axis=0), -1, f) for f in filters2]) for fm in fmap1_test])\n",
    "\n",
    "    print(\"🔐 Binary hashing + histogram (train)...\")\n",
    "    hashed_train = binary_hashing(fmap2_train)\n",
    "    features_train = block_histogram(hashed_train, block_size=(7, 7), overlap=0.5)\n",
    "\n",
    "    print(\"🔐 Binary hashing + histogram (test)...\")\n",
    "    hashed_test = binary_hashing(fmap2_test)\n",
    "    features_test = block_histogram(hashed_test, block_size=(7, 7), overlap=0.5)\n",
    "\n",
    "    print(\"🏷 Training Linear SVM...\")\n",
    "    clf = LinearSVC(max_iter=3000)\n",
    "    clf.fit(features_train, y_train)\n",
    "    y_pred = clf.predict(features_test)\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    print(f\"✅ Final Test Accuracy: {acc * 100:.2f}%\")\n",
    "\n",
    "# Jalankan pipeline pada file mnist_test.csv\n",
    "run_pcn_from_csv(\"mnist_test.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
