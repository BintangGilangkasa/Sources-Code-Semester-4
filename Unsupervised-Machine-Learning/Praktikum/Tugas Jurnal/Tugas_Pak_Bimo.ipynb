{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Bvx58_WzFctK"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "from torchvision.datasets import MNIST\n",
        "from torchvision import transforms\n",
        "from scipy.signal import convolve2d\n",
        "import warnings\n",
        "import cv2\n",
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "r0nrAjouRKMD"
      },
      "outputs": [],
      "source": [
        "warnings.filterwarnings('ignore')  # Biarkan output bersih\n",
        "\n",
        "# Load subset MNIST\n",
        "def load_mnist(max_train=3000, max_test=500):\n",
        "    mnist = fetch_openml('mnist_784', version=1, as_frame=False)\n",
        "    X = mnist.data.reshape(-1, 28, 28).astype(np.float32)\n",
        "    y = mnist.target.astype(int)\n",
        "\n",
        "    # Split dulu sebagian untuk test\n",
        "    X_temp, X_test, y_temp, y_test = train_test_split(X, y, test_size=max_test, stratify=y, random_state=42)\n",
        "    # Lalu ambil max_train dari sisanya\n",
        "    X_train, _, y_train, _ = train_test_split(X_temp, y_temp, train_size=max_train, stratify=y_temp, random_state=42)\n",
        "\n",
        "    return X_train, y_train, X_test, y_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "XHSYdCfbRMlJ"
      },
      "outputs": [],
      "source": [
        "# Patch extraction acak (hemat memori)\n",
        "def extract_random_patches(images, patch_size=7, stride=1, max_patches=50000):\n",
        "    n, h, w = images.shape\n",
        "    k = patch_size\n",
        "    patch_vectors = []\n",
        "\n",
        "    for idx in range(n):\n",
        "        for i in range(0, h - k + 1, stride):\n",
        "            for j in range(0, w - k + 1, stride):\n",
        "                patch = images[idx, i:i + k, j:j + k]\n",
        "                patch = patch - np.mean(patch)\n",
        "                patch_vectors.append(patch.flatten())\n",
        "                if len(patch_vectors) >= max_patches:\n",
        "                    return np.array(patch_vectors).T\n",
        "    return np.array(patch_vectors).T"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "boC3EwK-RSQ9"
      },
      "outputs": [],
      "source": [
        "# PCA filter\n",
        "def learn_pca_filters(patches, num_filters, patch_size):\n",
        "    print(f\"Learning PCA filters with shape {patches.shape}...\")\n",
        "    pca = PCA(n_components=num_filters)\n",
        "    pca.fit(patches.T)\n",
        "    filters = pca.components_.reshape((num_filters, patch_size, patch_size))\n",
        "    return filters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "qp6SHo1_RTLK"
      },
      "outputs": [],
      "source": [
        "# Convolve images\n",
        "def convolve_images(images, filters):\n",
        "    feature_maps = []\n",
        "    for f in filters:\n",
        "        fmaps = [convolve2d(img, f, mode='valid') for img in images]\n",
        "        feature_maps.append(np.stack(fmaps))\n",
        "    return feature_maps  # List of arrays shape (n, h', w')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Dra6TCUpRTaW"
      },
      "outputs": [],
      "source": [
        "# Binary hashing\n",
        "def binary_hashing(feature_maps):\n",
        "    bin_stack = np.stack([(fm > 0).astype(np.uint8) for fm in feature_maps], axis=0)\n",
        "    powers = 2 ** np.arange(bin_stack.shape[0])[::-1].reshape((-1, 1, 1, 1))\n",
        "    hashed = np.sum(bin_stack * powers, axis=0)\n",
        "    return hashed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "yVfljWY6RTpZ"
      },
      "outputs": [],
      "source": [
        "# Histogram encoding\n",
        "def block_histogram(images, block_size=(7, 7), num_bins=256, overlap=0.5):\n",
        "    n, h, w = images.shape\n",
        "    bh, bw = block_size\n",
        "    step_h = int(bh * (1 - overlap))\n",
        "    step_w = int(bw * (1 - overlap))\n",
        "\n",
        "    features = []\n",
        "    for img in images:\n",
        "        blocks = []\n",
        "        for i in range(0, h - bh + 1, step_h):\n",
        "            for j in range(0, w - bw + 1, step_w):\n",
        "                block = img[i:i + bh, j:j + bw]\n",
        "                hist, _ = np.histogram(block, bins=num_bins, range=(0, num_bins))\n",
        "                blocks.extend(hist)\n",
        "        features.append(np.array(blocks))\n",
        "    return np.stack(features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dRfJCE-bRfqL",
        "outputId": "c5c6bb58-e291-4f8c-e630-59a4406260db"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🔍 Loading MNIST...\n",
            "🧩 Extracting stage-1 patches...\n",
            "🔧 Training stage-1 PCA filters...\n",
            "Learning PCA filters with shape (49, 50000)...\n",
            "🎛 Convolution stage-1...\n",
            "🧠 Extracting stage-2 patches (subset only)...\n",
            "🔧 Training stage-2 PCA filters...\n",
            "Learning PCA filters with shape (49, 50000)...\n",
            "🎛 Convolution stage-2...\n",
            "🔐 Binary hashing + histogram (train)...\n",
            "🧪 Applying model to test set...\n",
            "🏷 Training Linear SVM...\n",
            "✅ Final Test Accuracy: 92.40%\n"
          ]
        }
      ],
      "source": [
        "# Main pipeline\n",
        "def run_pcn_mnist():\n",
        "    print(\"🔍 Loading MNIST...\")\n",
        "    X_train, y_train, X_test, y_test = load_mnist(max_train=3000, max_test=500)\n",
        "\n",
        "    print(\"🧩 Extracting stage-1 patches...\")\n",
        "    patches1 = extract_random_patches(X_train, patch_size=7, stride=1, max_patches=50000)\n",
        "\n",
        "    print(\"🔧 Training stage-1 PCA filters...\")\n",
        "    filters1 = learn_pca_filters(patches1, num_filters=6, patch_size=7)\n",
        "\n",
        "    print(\"🎛 Convolution stage-1...\")\n",
        "    # fmap1 will be a list of arrays, each array is shape (n, h', w') for one filter\n",
        "    fmap1 = convolve_images(X_train, filters1)\n",
        "\n",
        "    print(\"🧠 Extracting stage-2 patches (subset only)...\")\n",
        "    # Need patches from stage 1 feature maps across images and filters\n",
        "    # fmap1 is a list of (n, h', w') arrays. Stack them to (num_filters1, n, h', w') then reshape to (num_filters1*n, h', w')\n",
        "    fmap1_stacked_for_patches = np.concatenate(fmap1, axis=0) # shape (num_filters1 * n, h', w')\n",
        "    fmap1_stacked_for_patches = fmap1_stacked_for_patches[:3000] # Take a subset\n",
        "    patches2 = extract_random_patches(fmap1_stacked_for_patches, patch_size=7, stride=1, max_patches=50000)\n",
        "\n",
        "\n",
        "    print(\"🔧 Training stage-2 PCA filters...\")\n",
        "    filters2 = learn_pca_filters(patches2, num_filters=11, patch_size=7)\n",
        "\n",
        "    print(\"🎛 Convolution stage-2...\")\n",
        "    fmap2 = []\n",
        "    # fmap1 is a list of arrays, each shape (n, h', w')\n",
        "    # We need to iterate through images (n) and for each image,\n",
        "    # combine its feature maps from stage 1 (across filters1)\n",
        "    # and convolve with filters2.\n",
        "\n",
        "    # Reshape fmap1 from list of (n, h', w') to array (n, num_filters1, h', w')\n",
        "    fmap1_stacked = np.stack(fmap1, axis=1) # shape (n, num_filters1, h', w')\n",
        "\n",
        "    for i in range(fmap1_stacked.shape[0]): # Iterate through images (n)\n",
        "        # For each image, combine feature maps from different filters1\n",
        "        img_fmaps1_combined = fmap1_stacked[i].sum(axis=0) # shape (h', w')\n",
        "        # Convolve the combined map for this image with each filter2\n",
        "        conv_maps = [cv2.filter2D(img_fmaps1_combined, -1, f) for f in filters2]\n",
        "        fmap2.append(np.stack(conv_maps)) # shape (num_filters2, h'', w'')\n",
        "    fmap2 = np.array(fmap2) # Final shape (n, num_filters2, h'', w'')\n",
        "\n",
        "    # The binary hashing and histogram functions expect input with samples as the first dimension.\n",
        "    # binary_hashing is currently implemented to hash along the *first* dimension.\n",
        "    # To hash across filters (axis=1 in fmap2's final shape), we need to transpose\n",
        "    # the input for binary_hashing.\n",
        "    fmap2_hashed_input = np.transpose(fmap2, (1, 0, 2, 3)) # shape (num_filters2, n, h'', w'')\n",
        "\n",
        "    print(\"🔐 Binary hashing + histogram (train)...\")\n",
        "    # Pass the correctly shaped input to binary_hashing\n",
        "    hashed_train = binary_hashing(fmap2_hashed_input) # shape (n, h'', w'')\n",
        "    features_train = block_histogram(hashed_train, block_size=(7, 7), overlap=0.5)\n",
        "\n",
        "\n",
        "    print(\"🧪 Applying model to test set...\")\n",
        "    fmap1_test = convolve_images(X_test, filters1)\n",
        "\n",
        "    # Apply the same logic as the training set convolution for stage 2\n",
        "    fmap1_test_stacked = np.stack(fmap1_test, axis=1) # shape (n_test, num_filters1, h', w')\n",
        "\n",
        "    fmap2_test = []\n",
        "    for i in range(fmap1_test_stacked.shape[0]): # Iterate through test images (n_test)\n",
        "        img_fmaps1_test_combined = fmap1_test_stacked[i].sum(axis=0) # shape (h', w')\n",
        "        conv_maps_test = [cv2.filter2D(img_fmaps1_test_combined, -1, f) for f in filters2]\n",
        "        fmap2_test.append(np.stack(conv_maps_test))\n",
        "    fmap2_test = np.array(fmap2_test) # Final shape (n_test, num_filters2, h'', w'')\n",
        "\n",
        "    # Transpose for binary hashing\n",
        "    fmap2_test_hashed_input = np.transpose(fmap2_test, (1, 0, 2, 3)) # shape (num_filters2, n_test, h'', w'')\n",
        "\n",
        "    hashed_test = binary_hashing(fmap2_test_hashed_input) # shape (n_test, h'', w'')\n",
        "    features_test = block_histogram(hashed_test, block_size=(7, 7), overlap=0.5)\n",
        "\n",
        "    print(\"🏷 Training Linear SVM...\")\n",
        "    clf = LinearSVC(max_iter=3000)\n",
        "    # features_train should now have shape (n, num_features_per_image) where n is 3000\n",
        "    # features_test should have shape (n_test, num_features_per_image) where n_test is 500\n",
        "    # y_train has shape (3000,) and y_test has shape (500,)\n",
        "    # This should resolve the inconsistent sample error.\n",
        "    clf.fit(features_train, y_train)\n",
        "    y_pred = clf.predict(features_test)\n",
        "    acc = accuracy_score(y_test, y_pred)\n",
        "    print(f\"✅ Final Test Accuracy: {acc * 100:.2f}%\")\n",
        "\n",
        "run_pcn_mnist()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
