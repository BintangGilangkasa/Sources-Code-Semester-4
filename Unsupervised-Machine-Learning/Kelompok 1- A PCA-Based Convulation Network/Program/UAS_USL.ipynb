{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "25d76716",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import Libraries\n",
    "import numpy as np\n",
    "import cv2\n",
    "import joblib\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "47611e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === LOAD MNIST ===\n",
    "def load_mnist_data(max_samples=3000):\n",
    "    mnist = fetch_openml('mnist_784', version=1)\n",
    "    X = mnist.data.to_numpy().astype(np.float32).reshape(-1, 28, 28)\n",
    "    y = mnist.target.astype(int)\n",
    "    return X[:max_samples], y[:max_samples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9f36478b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === EXTRACT PATCHES ===\n",
    "def extract_random_patches(images, patch_size=7, stride=1, max_patches=50000):\n",
    "    patches = []\n",
    "    for img in images:\n",
    "        for i in range(0, img.shape[0] - patch_size + 1, stride):\n",
    "            for j in range(0, img.shape[1] - patch_size + 1, stride):\n",
    "                patch = img[i:i+patch_size, j:j+patch_size]\n",
    "                patch = patch - np.mean(patch)\n",
    "                patches.append(patch.flatten())\n",
    "                if len(patches) >= max_patches:\n",
    "                    return np.array(patches)\n",
    "    return np.array(patches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8237c65b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === LEARN PCA FILTERS ===\n",
    "def learn_pca_filters(patches, num_filters, patch_size):\n",
    "    pca = PCA(n_components=num_filters)\n",
    "    pca.fit(patches)\n",
    "    filters = pca.components_.reshape((num_filters, patch_size, patch_size))\n",
    "    return filters, pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "acb0dfe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CONVOLVE ===\n",
    "def convolve_images(images, filters):\n",
    "    fmap_all = []\n",
    "    for img in images:\n",
    "        maps = [cv2.filter2D(img, -1, f) for f in filters]\n",
    "        fmap_all.append(np.stack(maps))\n",
    "    return np.array(fmap_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "98a9a472",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === BINARY HASHING + HISTOGRAM ===\n",
    "def binary_hashing(feature_maps):\n",
    "    # feature_maps: shape (n_samples, n_filters, H, W)\n",
    "    all_hashed = []\n",
    "    for fmap in feature_maps:\n",
    "        bin_stack = (fmap > 0).astype(np.uint8)  # (n_filters, H, W)\n",
    "        powers = 2 ** np.arange(bin_stack.shape[0])[::-1].reshape((-1, 1, 1))\n",
    "        hashed = np.sum(bin_stack * powers, axis=0)  # (H, W)\n",
    "        all_hashed.append(hashed)\n",
    "    return np.array(all_hashed)  # shape (n_samples, H, W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6390a4f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === BLOCK HISTOGRAM ===\n",
    "def block_histogram(images, block_size=(7, 7), num_bins=256, overlap=0.5):\n",
    "    n, h, w = images.shape\n",
    "    bh, bw = block_size\n",
    "    step_h = int(bh * (1 - overlap))\n",
    "    step_w = int(bw * (1 - overlap))\n",
    "    features = []\n",
    "    for img in images:\n",
    "        blocks = []\n",
    "        for i in range(0, h - bh + 1, step_h):\n",
    "            for j in range(0, w - bw + 1, step_w):\n",
    "                block = img[i:i + bh, j:j + bw]\n",
    "                hist, _ = np.histogram(block, bins=num_bins, range=(0, num_bins))\n",
    "                blocks.extend(hist)\n",
    "        features.append(np.array(blocks))\n",
    "    return np.stack(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5d3e73e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📥 Loading data...\n",
      "🧩 Stage 1 PCA...\n",
      "🎛 Convolution stage 1...\n",
      "🧩 Stage 2 PCA...\n",
      "🎛 Convolution stage 2...\n",
      "🔐 Feature encoding...\n",
      "🎯 Training classifier...\n",
      "✅ Accuracy: 95.50%\n",
      "💾 Saving model...\n",
      "✅ Model saved as pcn_model.joblib\n"
     ]
    }
   ],
   "source": [
    "# === TRAIN + SAVE MODEL ===\n",
    "def train_and_save_model():\n",
    "    print(\"📥 Loading data...\")\n",
    "    X, y = load_mnist_data(3000)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8, stratify=y, random_state=42)\n",
    "\n",
    "    print(\"🧩 Stage 1 PCA...\")\n",
    "    patches1 = extract_random_patches(X_train, patch_size=7, stride=1, max_patches=50000)\n",
    "    filters1, pca1 = learn_pca_filters(patches1, num_filters=6, patch_size=7)\n",
    "\n",
    "    print(\"🎛 Convolution stage 1...\")\n",
    "    fmap1_train = convolve_images(X_train, filters1)\n",
    "    fmap1_test = convolve_images(X_test, filters1)\n",
    "\n",
    "    print(\"🧩 Stage 2 PCA...\")\n",
    "    combined_maps = np.array([np.sum(fm, axis=0) for fm in fmap1_train])\n",
    "    patches2 = extract_random_patches(combined_maps, patch_size=7, stride=1, max_patches=50000)\n",
    "    filters2, pca2 = learn_pca_filters(patches2, num_filters=11, patch_size=7)\n",
    "\n",
    "    print(\"🎛 Convolution stage 2...\")\n",
    "    fmap2_train = np.array([np.stack([cv2.filter2D(np.sum(fm, axis=0), -1, f) for f in filters2]) for fm in fmap1_train])\n",
    "    fmap2_test = np.array([np.stack([cv2.filter2D(np.sum(fm, axis=0), -1, f) for f in filters2]) for fm in fmap1_test])\n",
    "\n",
    "    print(\"🔐 Feature encoding...\")\n",
    "    hashed_train = binary_hashing(fmap2_train)\n",
    "    hashed_test = binary_hashing(fmap2_test)\n",
    "    features_train = block_histogram(hashed_train, block_size=(7, 7), overlap=0.5)\n",
    "    features_test = block_histogram(hashed_test, block_size=(7, 7), overlap=0.5)\n",
    "\n",
    "    print(\"🎯 Training classifier...\")\n",
    "    clf = LinearSVC(max_iter=3000)\n",
    "    clf.fit(features_train, y_train)\n",
    "\n",
    "    acc = clf.score(features_test, y_test)\n",
    "    print(f\"✅ Accuracy: {acc * 100:.2f}%\")\n",
    "\n",
    "    print(\"💾 Saving model...\")\n",
    "    joblib.dump({'filters1': filters1, 'filters2': filters2, 'clf': clf}, \"pcn_model.joblib\")\n",
    "    print(\"✅ Model saved as pcn_model.joblib\")\n",
    "\n",
    "train_and_save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b9cb867a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "# Mengubah gambar ke format yang sesuai\n",
    "def split_digits(image_path, save_folder=\".\", resize_to=(28, 28)):\n",
    "    # Baca dan ubah ke grayscale\n",
    "    img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "    if img is None:\n",
    "        raise ValueError(\"Gambar tidak ditemukan\")\n",
    "\n",
    "    # Invers warna agar angka jadi putih\n",
    "    img_inv = cv2.bitwise_not(img)\n",
    "\n",
    "    # Threshold biner\n",
    "    _, thresh = cv2.threshold(img_inv, 50, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "    # Cari kontur angka\n",
    "    contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    # Urutkan kontur dari kiri ke kanan\n",
    "    bounding_boxes = [cv2.boundingRect(c) for c in contours]\n",
    "    bounding_boxes = sorted(bounding_boxes, key=lambda b: b[0])  # urut x\n",
    "\n",
    "    # Potong dan simpan setiap angka\n",
    "    filenames = []\n",
    "    for i, (x, y, w, h) in enumerate(bounding_boxes):\n",
    "        digit = img[y:y+h, x:x+w]\n",
    "        digit = 255 - digit  # Invers: putih jadi hitam, hitam jadi putih\n",
    "        digit_resized = cv2.resize(digit, resize_to)\n",
    "        filename = f\"{save_folder}/digit_{i}.png\"\n",
    "        cv2.imwrite(filename, digit_resized)\n",
    "        filenames.append(filename)\n",
    "\n",
    "    return filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5d1eada2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📸 Membaca gambar dari: ./digit_0.png\n",
      "🔢 Prediksi angka: 2\n"
     ]
    }
   ],
   "source": [
    "# === Fungsi deteksi gambar ===\n",
    "def predict_from_image(image_path, model_path=\"pcn_model.joblib\"):\n",
    "    print(f\"📸 Membaca gambar dari: {image_path}\")\n",
    "    img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "    if img is None:\n",
    "        raise ValueError(\"Gambar tidak ditemukan atau format tidak didukung.\")\n",
    "    \n",
    "    # Resize ke 28x28\n",
    "    img = cv2.resize(img, (28, 28)).astype(np.float32)\n",
    "\n",
    "    # Load model\n",
    "    model = joblib.load(model_path)\n",
    "    filters1 = model['filters1']\n",
    "    filters2 = model['filters2']\n",
    "    clf = model['clf']\n",
    "\n",
    "    # Stage 1\n",
    "    fmap1 = np.stack([cv2.filter2D(img, -1, f) for f in filters1])\n",
    "\n",
    "    # Stage 2\n",
    "    combined = np.sum(fmap1, axis=0)\n",
    "    fmap2 = np.stack([cv2.filter2D(combined, -1, f) for f in filters2])\n",
    "    fmap2 = np.expand_dims(fmap2, axis=0)\n",
    "\n",
    "    # Binary hashing + histogram\n",
    "    # Hash + histogram\n",
    "    hashed = binary_hashing(fmap2)         # hasil (1, 28, 28)\n",
    "    feature = block_histogram(hashed)[0]   # ambil histogram     \n",
    "\n",
    "    # Predict\n",
    "    pred = clf.predict([feature])[0]\n",
    "    print(f\"🔢 Prediksi angka: {pred}\")\n",
    "    return pred\n",
    "\n",
    "# Tempat untuk mencoba prediksi\n",
    "files = split_digits(\"7.jpg\")\n",
    "for f in files:\n",
    "    predict_from_image(f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
